structure = {
    "1_Preface_to_the_Second_Edition": [],
    "2_Preface_to_the_First_Edition": [],
    "3_Introduction": {
        "3.1_Examples_of_Learning_Problems": [],
        "3.2_Supervised_and_Unsupervised_Learning": [],
        "3.3_Statistical_Decision_Theory": [],
        "3.4_The_Many_Faces_of_Regularization": []
    },
    "4_Overview_of_Supervised_Learning": {
        "4.1_Introduction": [],
        "4.2_Variable_Types_and_Terminology": [],
        "4.3_Two_Simple_Approaches_to_Prediction:_Least_Squares_and_Nearest_Neighbors": [],
        "4.4_Statistical_Decision_Theory": [],
        "4.5_Local_Methods_in_High_Dimensions": [],
        "4.6_Statistical_Models,_Supervised_Learning_and_Function_Approximation": [],
        "4.7_Structured_Regression_Models": [],
        "4.8_Classes_of_Restricted_Estimators": [],
        "4.9_Model_Selection_and_the_Bias–Variance_Tradeoff": [],
        "4.10_Bibliographic_Notes": [],
        "4.11_Exercises": []
    },
    "5_Linear_Methods_for_Regression": {
        "5.1_Introduction": [],
        "5.2_Linear_Regression_Models_and_Least_Squares": [],
        "5.3_Subset_Selection": [],
        "5.4_Shrinkage_Methods": [],
        "5.5_Methods_Using_Derived_Input_Directions": [],
        "5.6_Discussion:_A_Comparison_of_the_Selection_and_Shrinkage_Methods": [],
        "5.7_Multiple_Outcome_Shrinkage_and_Selection": [],
        "5.8_More_on_the_Lasso_and_Related_Path_Algorithms": [],
        "5.9_Computational_Considerations": [],
        "5.10_Bibliographic_Notes": [],
        "5.11_Exercises": []
    },
    "6_Linear_Methods_for_Classification": {
        "6.1_Introduction": [],
        "6.2_Linear_Regression_of_an_Indicator_Matrix": [],
        "6.3_Linear_Discriminant_Analysis": [],
        "6.4_Logistic_Regression": [],
        "6.5_Separating_Hyperplanes": [],
        "6.6_Bibliographic_Notes": [],
        "6.7_Exercises": []
    },
    "7_Basis_Expansions_and_Regularization": {
        "7.1_Introduction": [],
        "7.2_Piecewise_Polynomials_and_Splines": [],
        "7.3_Filtering_and_Feature_Extraction": [],
        "7.4_Smoothing_Splines": [],
        "7.5_Nonparametric_Logistic_Regression": [],
        "7.6_Multidimensional_Splines": [],
        "7.7_Regularization_and_Reproducing_Kernel_Hilbert_Spaces": [],
        "7.8_Wavelet_Smoothing": [],
        "7.9_Bibliographic_Notes": [],
        "7.10_Exercises": [],
        "7.11_Appendix:_Computational_Considerations_for_Splines": []
    },
    "8_Kernel_Smoothing_Methods": {
        "8.1_One-Dimensional_Kernel_Smoothers": [],
        "8.2_Selecting_the_Width_of_the_Kernel": [],
        "8.3_Local_Regression_in_Rp": [],
        "8.4_Structured_Local_Regression_Models_in_Rp": [],
        "8.5_Local_Likelihood_and_Other_Models": [],
        "8.6_Kernel_Density_Estimation_and_Classification": [],
        "8.7_Radial_Basis_Functions_and_Kernels": [],
        "8.8_Mixture_Models_for_Density_Estimation_and_Classification": [],
        "8.9_Computational_Considerations": [],
        "8.10_Bibliographic_Notes": [],
        "8.11_Exercises": []
    },
    "9_Model_Assessment_and_Selection": {
        "9.1_Introduction": [],
        "9.2_Bias,_Variance_and_Model_Complexity": [],
        "9.3_The_Bias–Variance_Decomposition": [],
        "9.4_Optimism_of_the_Training_Error_Rate": [],
        "9.5_Estimates_of_In-Sample_Prediction_Error": [],
        "9.6_The_Effective_Number_of_Parameters": [],
        "9.7_The_Bayesian_Approach_and_BIC": [],
        "9.8_Minimum_Description_Length": [],
        "9.9_Vapnik–Chervonenkis_Dimension": [],
        "9.10_Cross-Validation": [],
        "9.11_Bootstrap_Methods": [],
        "9.12_Conditional_or_Expected_Test_Error?": [],
        "9.13_Bibliographic_Notes": [],
        "9.14_Exercises": []
    },
    "10_Model_Inference_and_Averaging": {
        "10.1_Introduction": [],
        "10.2_The_Bootstrap_and_Maximum_Likelihood_Methods": [],
        "10.3_Bayesian_Methods": [],
        "10.4_Relationship_Between_the_Bootstrap_and_Bayesian_Inference": [],
        "10.5_The_EM_Algorithm": [],
        "10.6_MCMC_for_Sampling_from_the_Posterior": [],
        "10.7_Bagging": [],
        "10.8_Model_Averaging_and_Stacking": [],
        "10.9_Stochastic_Search:_Bumping": [],
        "10.10_Bibliographic_Notes": [],
        "10.11_Exercises": []
    },
    "11_Additive_Models,_Trees,_and_Related_Methods": {
        "11.1_Generalized_Additive_Models": [],
        "11.2_Tree-Based_Methods": [],
        "11.3_PRIM:_Bump_Hunting": [],
        "11.4_MARS:_Multivariate_Adaptive_Regression_Splines": [],
        "11.5_Hierarchical_Mixtures_of_Experts": [],
        "11.6_Missing_Data": [],
        "11.7_Computational_Considerations": [],
        "11.8_Bibliographic_Notes": [],
        "11.9_Exercises": []
    },
    "12_Boosting_and_Additive_Trees": {
        "12.1_Boosting_Methods": [],
        "12.2_Boosting_Fits_an_Additive_Model": [],
        "12.3_Forward_Stagewise_Additive_Modeling": [],
        "12.4_Exponential_Loss_and_AdaBoost": [],
        "12.5_Why_Exponential_Loss?": [],
        "12.6_Loss_Functions_and_Robustness": [],
        "12.7_Off-the-Shelf_Procedures_for_Data_Mining": [],
        "12.8_Example:_Spam_Data": [],
        "12.9_Boosting_Trees": [],
        "12.10_Numerical_Optimization_via_Gradient_Boosting": [],
        "12.11_Right-Sized_Trees_for_Boosting": [],
        "12.12_Regularization": [],
        "12.13_Interpretation": [],
        "12.14_Illustrations": [],
        "12.15_Bibliographic_Notes": [],
        "12.16_Exercises": []
    },
    "13_Neural_Networks": {
        "13.1_Introduction": [],
        "13.2_Projection_Pursuit_Regression": [],
        "13.3_Neural_Networks": [],
        "13.4_Fitting_Neural_Networks": [],
        "13.5_Some_Issues_in_Training_Neural_Networks": [],
        "13.6_Example:_Simulated_Data": [],
        "13.7_Example:_ZIP_Code_Data": [],
        "13.8_Discussion": [],
        "13.9_Bayesian_Neural_Nets_and_the_NIPS_2003_Challenge": [],
        "13.10_Computational_Considerations": [],
        "13.11_Bibliographic_Notes": [],
        "13.12_Exercises": []
    },
    "14_Support_Vector_Machines_and_Flexible_Discriminants": {
        "14.1_Introduction": [],
        "14.2_The_Support_Vector_Classifier": [],
        "14.3_Support_Vector_Machines_and_Kernels": [],
        "14.4_Generalizing_Linear_Discriminant_Analysis": [],
        "14.5_Flexible_Discriminant_Analysis": [],
        "14.6_Penalized_Discriminant_Analysis": [],
        "14.7_Mixture_Discriminant_Analysis": [],
        "14.8_Bibliographic_Notes": [],
        "14.9_Exercises": []
    },
    "15_Prototype_Methods_and_Nearest-Neighbors": {
        "15.1_Introduction": [],
        "15.2_Prototype_Methods": [],
        "15.3_k-Nearest-Neighbor_Classifiers": [],
        "15.4_Adaptive_Nearest-Neighbor_Methods": [],
        "15.5_Computational_Considerations": [],
        "15.6_Bibliographic_Notes": [],
        "15.7_Exercises": []
    },
    "16_Unsupervised_Learning": {
        "16.1_Introduction": [],
        "16.2_Association_Rules": [],
        "16.3_Cluster_Analysis": [],
        "16.4_Self-Organizing_Maps": [],
        "16.5_Principal_Components,_Curves_and_Surfaces": [],
        "16.6_Non-negative_Matrix_Factorization": [],
        "16.7_Independent_Component_Analysis_and_Exploratory_Projection_Pursuit": [],
        "16.8_Multidimensional_Scaling": [],
        "16.9_Nonlinear_Dimension_Reduction_and_Local_Multidimensional_Scaling": [],
        "16.10_The_Google_PageRank_Algorithm": [],
        "16.11_Bibliographic_Notes": [],
        "16.12_Exercises": []
    },
    "17_Random_Forests": {
        "17.1_Introduction": [],
        "17.2_Definition_of_Random_Forests": [],
        "17.3_Details_of_Random_Forests": [],
        "17.4_Analysis_of_Random_Forests": [],
        "17.5_Bibliographic_Notes": [],
        "17.6_Exercises": []
    },
    "18_Ensemble_Learning": {
        "18.1_Introduction": [],
        "18.2_Boosting_and_Regularization_Paths": [],
        "18.3_Learning_Ensembles": [],
        "18.4_Bibliographic_Notes": [],
        "18.5_Exercises": []
    },
    "19_Undirected_Graphical_Models": {
        "19.1_Introduction": [],
        "19.2_Markov_Graphs_and_Their_Properties": [],
        "19.3_Undirected_Graphical_Models_for_Continuous_Variables": [],
        "19.4_Undirected_Graphical_Models_for_Discrete_Variables": [],
        "19.5_Exercises": []
    },
    "20_High-Dimensional_Problems:_p_greater_than_N": {
        "20.1_When_p_is_Much_Bigger_than_N": [],
        "20.2_Diagonal_Linear_Discriminant_Analysis_and_Nearest_Shrunken_Centroids": [],
        "20.3_Linear_Classifiers_with_Quadratic_Regularization": [],
        "20.4_Linear_Classifiers_with_L1_Regularization": [],
        "20.5_Classification_When_Features_are_Unavailable": [],
        "20.6_High-Dimensional_Regression:_Supervised_Principal_Components": [],
        "20.7_Feature_Assessment_and_the_Multiple-Testing_Problem": [],
        "20.8_Bibliographic_Notes": [],
        "20.9_Exercises": []
    },
    "21_References": [],
    "22_Author_Index": [],
    "23_Index": []
}



import os
import json
from typing import Union, Dict, List, Any

def create_directories_and_files(
    base_path: str, 
    structure: Dict[str, Any], 
    readme_file, 
    parent_path: str = "", 
    level: int = 1
):
    
    """

        根据给定的目录结构创建目录和文件，并生成 README.md 文件。

        Args:
            base_path (str): 根目录路径。
            structure (Dict[str, Any]): 目录结构的嵌套字典。
            readme_file (File): 用于写入README内容的文件对象。
            parent_path (str): 父目录路径。
            level (int): 目录的层级，用于确定 README 标题级别。

        Returns:
            None
        
    """

    heading = "#" * level

    for key, value in structure.items():
        current_path = os.path.join(base_path, key.replace(" ", "_").replace("-", "_"))

        # 创建目录
        os.makedirs(current_path, exist_ok=True)

        # 在README中添加章节标题
        if parent_path:
            readme_file.write(f"{heading} {parent_path}/{key}\n\n")
        else:
            readme_file.write(f"{heading} {key}\n\n")

        # 递归调用创建子目录和文件
        if isinstance(value, dict) and value:
            create_directories_and_files(
                current_path, 
                value, 
                readme_file, 
                parent_path + "/" + key if parent_path else key, 
                level + 1
            )
        elif isinstance(value, list) and value:
            for idx, item in enumerate(value):
                item = f"{idx:02d}_{item}"
                file_name = item.replace(" ", "_").replace("-", "_") + ".py"
                file_path = os.path.join(current_path, file_name)
                with open(file_path, 'w', encoding='utf-8') as file:
                    file.write(f"# {item}\n\n")
                    file.write(f'"""\n\nLecture: {parent_path}/{key}\nContent: {item}\n\n"""\n\n')

                # 在README中添加文件链接
                item_clean = item.replace(" ", "_").replace("-", "_")
                parent_clean = parent_path.replace(" ", "_").replace("-", "_")
                key_clean = key.replace(" ", "_").replace("-", "_")
                readme_file.write(f"- [{item}](./{parent_clean}/{key_clean}/{item_clean}.md)\n")
                readme_file.write(f"- [{item}](./{parent_clean}/{key_clean}/{item_clean}.py)\n")
        else:
            item_clean = key.replace(" ", "_").replace("-", "_")
            # 创建文件并写入初始内容
            file_name = item_clean + ".py"
            file_path = os.path.join(current_path, file_name)
            with open(file_path, 'w', encoding='utf-8') as file:
                file.write(f"# {key}\n\n")
                file.write(f'"""\n\nLecture: {parent_path}/{key}\nContent: {key}\n\n"""\n\n')
                
                
            file_name = item_clean + ".md"
            file_path = os.path.join(current_path, file_name)
            with open(file_path, 'w', encoding='utf-8') as file:
                file.write(f"# {key}\n\n")
                file.write(f'"""\n\nLecture: {parent_path}/{key}\nContent: {key}\n\n"""\n\n')

            # 在README中添加文件链接
            parent_clean = parent_path.replace(" ", "_").replace("-", "_")
            key_clean = key.replace(" ", "_").replace("-", "_")
            readme_file.write(f"- [{key}](./{parent_clean}/{key_clean}/{item_clean}.md)\n")
            readme_file.write(f"- [{key}](./{parent_clean}/{key_clean}/{item_clean}.py)\n")

        # 添加空行以分隔不同的章节
        readme_file.write("\n")

def main():
    root_dir = './'
    # 创建根目录
    os.makedirs(root_dir, exist_ok=True)

    # 创建 README.md 文件
    with open(os.path.join(root_dir, "README.md"), 'w', encoding='utf-8') as readme_file:
        readme_file.write("# The ELEMENTS OF STASTISTICAL LEARNING\n\n")
        readme_file.write("这是一个关于The ELEMENTS OF STASTISTICAL LEARNING的目录结构。\n\n")
        create_directories_and_files(root_dir, structure, readme_file)

    print("目录和文件结构已生成，并创建 README.md 文件。")

if __name__ == "__main__":
    main()
